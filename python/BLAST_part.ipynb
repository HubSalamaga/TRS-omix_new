{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cffi\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "from enum import Enum\n",
    "from Bio import SeqIO\n",
    "from Bio import Entrez\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from multiprocessing import Pool\n",
    "import glob\n",
    "import re\n",
    "import fnmatch\n",
    "from src.SequenceProcessor import SequenceProcessor\n",
    "from src.FileHandler import FileHandler\n",
    "from src.pytrsomix import SeqAnalyzer,TRScalculator\n",
    "from src.BlastProcessor import BLASTProcessor\n",
    "import argparse\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_accession_to_taxid(accession):\n",
    "    \"\"\"Map accession to TaxID using the optimized lookup dictionary.\"\"\"\n",
    "    return accession_to_taxid.get(accession, '') \n",
    " \n",
    "def main():\n",
    "    start_time = time.time()\n",
    "    parser = argparse.ArgumentParser(description='''This program analyzes the blast output files to find species specific sequences coming from genomes\n",
    "                                     analyzed in the previous step''')\n",
    "    parser.add_argument('--blast_output_path', help=\"Path to a folder containing blast results in a valid format\", type=str, required=True)\n",
    "    parser.add_argument('--taxonomy_db',help='Path to the taxonomy - accession database', required=True, type= str)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    blast_output = args.blast_output_path\n",
    "    taxonomy_db = args.taxonomy_db\n",
    "\n",
    "    FileHandler.convert_to_txt(blast_output)\n",
    "    FileHandler.filter_and_overwrite_blast_file(blast_output)\n",
    "    blast_output_path = Path(blast_output)\n",
    "    results_directory = blast_output_path.parent\n",
    "\n",
    "\n",
    "    '''\n",
    "    Get accessions present in blast files and filter the taxonomy_file using them this way we have all accession - taxid\n",
    "    Pairs in out dataset and there is not need to access such a large database.\n",
    "    In addition as you can see I'm using chunks to process the initial database, that size could be altered but i found \n",
    "    50000 to be sweet spot. Loading progress bar was added for this operation as well.\n",
    "    '''\n",
    "    taxonomy_file = taxonomy_db\n",
    "    accessions = BLASTProcessor.collect_accessions_from_blast_files(blast_output)\n",
    "    tax_df = SequenceProcessor.filter_taxonomy_file(taxonomy_file,accessions,50000)\n",
    "    \n",
    "    '''\n",
    "    Saving the filtered database to a csv file for use later - NOT IMPLEMENTED\n",
    "    '''\n",
    "    \n",
    "    taxonomy_dataframe = os.path.join(blast_output_path, \"taxonomy_filtered.csv\")\n",
    "    tax_df.to_csv(taxonomy_dataframe, index=False)\n",
    "    print(f\"tax_df saved to {taxonomy_dataframe}\")\n",
    "\n",
    "    '''\n",
    "    Quite self-explanatory makes sure that each taxid - accession pair gets put into dictionary \n",
    "    only once i make use of sets here to decrease it's size and repeated information\n",
    "    '''\n",
    "    taxid_accessions_dict = {}\n",
    "    for index, row in tax_df.iterrows():\n",
    "        accession_column = tax_df.columns[0]  # Extract accession column dynamically\n",
    "        taxid_column = tax_df.columns[1]  # Extract taxid column dynamically\n",
    "        \n",
    "        accession = row[accession_column]\n",
    "        taxid = row[taxid_column]\n",
    "        \n",
    "        if taxid in taxid_accessions_dict:\n",
    "            taxid_accessions_dict[taxid].append(accession)\n",
    "        else:\n",
    "            taxid_accessions_dict[taxid] = [accession]\n",
    "    # Ensure the directory for modified files exists\n",
    "    modified_blast_path = os.path.join(blast_output_path, \"modified_blast\")\n",
    "    FileHandler.ensure_directory_exists(modified_blast_path)  # Ensure this function creates the directory if it doesn't exist\n",
    "\n",
    "    '''\n",
    "    This step assumes each accession maps to exactly one taxid which is true (but that matching is not always accurate)\n",
    "    this drawback is attributed to nucleotide database structure and is not something that should alter the results in any\n",
    "    significant way as long as we remember to add the top-level species to the dictionary later\n",
    "    '''\n",
    "\n",
    "    accession_to_taxid = {accession: taxid for taxid, accession in taxid_accessions_dict.items() for accession in accessions}\n",
    "\n",
    "    # Iterate over files in the input directory\n",
    "    for filename in os.listdir(blast_output_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            input_file_path = os.path.join(blast_output_path, filename)\n",
    "            \n",
    "            # Read the file into a DataFrame\n",
    "            df = pd.read_csv(input_file_path, sep='\\t', header=None)  # No column names specified\n",
    "            \n",
    "            # Map accessions to TaxIDs and add them as a new column after the last existing column\n",
    "            df[df.shape[1]] = df[1].map(map_accession_to_taxid)  # Assuming column 1 contains the accessions\n",
    "            \n",
    "            # Construct output file path\n",
    "            modified_file_path = os.path.join(modified_blast_path, f\"taxids_{filename}\")\n",
    "            \n",
    "            # Save the modified DataFrame to a new file in the output directory\n",
    "            df.to_csv(modified_file_path, sep='\\t', index=False, header=False)\n",
    "    '''This is still experimental but should allow to load interiors.txt equivalent into a dataframe to construct the\n",
    "    dictionary of genomes which we analyze.'''\n",
    "    print(\"Searching for the results file....\")\n",
    "    results_file = FileHandler.find_file_by_name('*_results.csv',folder = results_directory, auto=True)\n",
    "    if results_file:\n",
    "        print(f\"Selected file: {results_file}\")\n",
    "    else:\n",
    "        # If results_file is either None or an empty string, prompt the user for the path.\n",
    "        print(\"No '*_results.csv' file selected or found.\")\n",
    "        results_file = input(\"Please provide the path to the current experiments result file: \")\n",
    "\n",
    "    combined_results = pd.read_csv(results_file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TRS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
